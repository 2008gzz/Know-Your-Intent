{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import codecs\n",
    "import spacy\n",
    "import sklearn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "#from os import *\n",
    "#import keras\n",
    "#import pandas as pd\n",
    "from sklearn import model_selection\n",
    "from time import time\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.neighbors.nearest_centroid import NearestCentroid\n",
    "from __future__ import unicode_literals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp=spacy.load('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#infra_path = os.path.join(\"data\", \"error_classification\", \"tocheck.txt\")\n",
    "#infra_path = os.path.join(\"data\", \"error_classification\", \"sample.txt\")\n",
    "# infra_path = os.path.join(\"data\", \"error_classification\", \"InfraRELATEDCHECK.txt\")\n",
    "# user_path = os.path.join(\"data\", \"error_classification\", \"USERRELATEDCHECK1.txt\")\n",
    "import json\n",
    "import csv\n",
    "intent_dict = {\"Make Update\":0, \"Setup Printer\":1, \"Shutdown Computer\":2, \"Software Recommendation\":3, \"None\":4}\n",
    "\n",
    "#intent_dict = {\"Make Update\":0, \"Setup Printer\":1, \"Shutdown Computer\":2, \"Software Recommendation\":3, \"None\":4}\n",
    "\n",
    "def read_CSV_datafile(filename):    \n",
    "    X = []\n",
    "    y = []\n",
    "    with open(filename, 'r') as csvfile:\n",
    "        reader = csv.reader(csvfile, delimiter='\\t')\n",
    "        for row in reader:\n",
    "            X.append(row[0])\n",
    "            y.append(intent_dict[row[1]])\n",
    "    return X,y\n",
    "        \n",
    "ubuntu_data_path = os.path.join(\"datasets\",\"NLU-Evaluation-Corpora\",\"AskUbuntuCorpus.json\")\n",
    "\n",
    "with open(ubuntu_data_path) as f:\n",
    "    ubuntu_data = json.load(f)\n",
    "    \n",
    "sentences = ubuntu_data[\"sentences\"]\n",
    "\n",
    "corpus = []\n",
    "y = []\n",
    "\n",
    "for sentence in sentences:\n",
    "    corpus.append(sentence[\"text\"])\n",
    "    y.append(intent_dict[sentence[\"intent\"]])\n",
    "# infra_data = [l.strip() for l in codecs.open(infra_path, \"r\", \"utf-8\")]\n",
    "# user_data = [l.strip() for l in codecs.open(user_path, \"r\", \"utf-8\")]\n",
    "\n",
    "#infra_data=codecs.open(infra_path,\"r\",\"utf-8\")\n",
    "\n",
    "#print len(serialize2)\n",
    "\n",
    "X_train_raw, y_train_raw = read_CSV_datafile(filename = \"datasets/KL/Ubuntu/train_new.csv\")\n",
    "X_test_raw, y_test_raw = read_CSV_datafile(filename = \"datasets/KL/Ubuntu/test.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corpus_data: \n",
      " ['Are there any Keyboard Shortcuts to Shutdown?', 'Shutdown after a certain time', 'Shutdown problem in Ubuntu 16.04', 'How do I fix a shutdown problem?'] \n",
      "\n",
      "\n",
      "y: \n",
      " [2, 2, 2, 2] \n",
      "\n",
      "\n",
      "Size of Corpus: 162\n",
      "Size of y: 162\n"
     ]
    }
   ],
   "source": [
    "print(\"corpus_data: \\n\",corpus[-5:-1], \"\\n\\n\")\n",
    "\n",
    "\n",
    "print(\"y: \\n\", y[-5:-1], \"\\n\\n\")\n",
    "\n",
    "print(\"Size of Corpus: {}\\nSize of y: {}\".format(len(corpus), len(y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stratified_split(X, y, n_splits=5, test_size=0.66):\n",
    "    skf = StratifiedShuffleSplit(n_splits=n_splits, test_size=test_size, random_state=0)\n",
    "    skf.get_n_splits(X, y)\n",
    "    splits = []\n",
    "    for train_index, test_index in skf.split(X, y):\n",
    "        print(\"TRAIN:\", train_index, \"\\n\\n\", \"TEST:\", test_index, \"\\n\\n\")\n",
    "        X_train, X_test = [X[i] for i in train_index], [X[i] for i in test_index]\n",
    "        y_train, y_test = [y[i] for i in train_index], [y[i] for i in test_index]\n",
    "        splits.append({\"train\": {\"X\": X_train, \"y\": y_train},\n",
    "                       \"test\": {\"X\": X_test, \"y\": y_test}})\n",
    "    return splits\n",
    "#print X_train[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(doc):\n",
    "    \"\"\"\n",
    "    Returns a list of strings containing each token in `sentence`\n",
    "    \"\"\"\n",
    "    #return [i for i in re.split(r\"([-.\\\"',:? !\\$#@~()*&\\^%;\\[\\]/\\\\\\+<>\\n=])\",\n",
    "    #                            doc) if i != '' and i != ' ' and i != '\\n']\n",
    "    tokens = []\n",
    "    doc = nlp.tokenizer(doc)\n",
    "    for token in doc:\n",
    "        #print(token.text, token.lemma_, token.pos_, token.tag_, token.dep_,\n",
    "        #      token.shape_, token.is_alpha, token.is_stop)\n",
    "        #if not token.is_stop:\n",
    "        #    clean_tokens.append(token.lemma_)\n",
    "        tokens.append(token.text)\n",
    "    return tokens\n",
    "\n",
    "\n",
    "def preprocess(doc):\n",
    "    clean_tokens = []\n",
    "    doc = nlp(doc)\n",
    "    for token in doc:\n",
    "        #print(token.text, token.lemma_, token.pos_, token.tag_, token.dep_,\n",
    "        #      token.shape_, token.is_alpha, token.is_stop)\n",
    "        if not token.is_stop:\n",
    "            clean_tokens.append(token.lemma_)\n",
    "        #clean_tokens.append(token.lemma_)\n",
    "    return \" \".join(clean_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SemHash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_ngrams(input_list, n):\n",
    "    return zip(*[input_list[i:] for i in range(n)])\n",
    "\n",
    "def semhash_tokenizer(text):\n",
    "    tokens = text.split(\" \")\n",
    "    final_tokens = []\n",
    "    for unhashed_token in tokens:\n",
    "        hashed_token = \"#{}#\".format(unhashed_token)\n",
    "        final_tokens += [''.join(gram)\n",
    "                         for gram in list(find_ngrams(list(hashed_token), 3))]\n",
    "    return final_tokens\n",
    "\n",
    "def semhash_corpus(corpus):\n",
    "    new_corpus = []\n",
    "    for sentence in corpus:\n",
    "        sentence = preprocess(sentence)\n",
    "        tokens = semhash_tokenizer(sentence)\n",
    "        new_corpus.append(\" \".join(map(str,tokens)))\n",
    "    return new_corpus\n",
    "\n",
    "corpus = semhash_corpus(corpus)\n",
    "\n",
    "X_train_raw = semhash_corpus(X_train)\n",
    "X_test_raw = semhash_corpus(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['#wh wha hat at# #so sof oft ftw twa war are re# #-P -PR PRO RON ON- N-# #us use se# #vi vie iew ew# #ep epu pub ub# #do doc ocu cum ume men ent nt# #?#',\n",
       " '#wh whi hic ich ch# #pd pdf df# #vi vie iew ewe wer er# #re rec eco com omm mme men end nd# #?#',\n",
       " '#wh wha hat at# #id ide de# #av ava vai ail ila lab abl ble le# #ub ubu bun unt ntu tu# #?#',\n",
       " '#wh wha hat at# #be be# #go goo ood od# #mi min ind nd# #ma map app ppi pin ing ng# #so sof oft ftw twa war are re# #?#',\n",
       " '#so sof oft ftw twa war are re# #re rea ead ad# #qr qr# #co cod ode de# #?#']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-PRON- fly kite man blue shirt try steal wallet\n"
     ]
    }
   ],
   "source": [
    "clean_doc = preprocess(\"I was flying a kite when the man in the blue shirt tried to steal my wallet\")\n",
    "print(clean_doc)\n",
    "#type(clean_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['-PRON-', 'fly', 'kite', 'man', 'blue', 'shirt', 'try', 'steal', 'wallet']\n"
     ]
    }
   ],
   "source": [
    "tokens_from_clean_text = tokenize(clean_doc)\n",
    "print(tokens_from_clean_text)\n",
    "#type(tokens_from_clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vectorizer(corpus, preprocessor=None, tokenizer=None):\n",
    "    vectorizer = CountVectorizer(ngram_range=(2,4),analyzer='char')\n",
    "    vectorizer.fit(corpus)\n",
    "    return vectorizer, vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import RidgeClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.linear_model import PassiveAggressiveClassifier\n",
    "from sklearn.naive_bayes import BernoulliNB, MultinomialNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neighbors import NearestCentroid\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.utils.extmath import density\n",
    "from sklearn import metrics\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "#from sklearn.cross_validation import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trim(s):\n",
    "    \"\"\"Trim string to fit on terminal (assuming 80-column display)\"\"\"\n",
    "    return s if len(s) <= 80 else s[:77] + \"...\"\n",
    "\n",
    "\n",
    "# #############################################################################\n",
    "# Benchmark classifiers\n",
    "def benchmark(clf, X_train, y_train, X_test, y_test, target_names,\n",
    "              print_report=True, feature_names=None, print_top10=False,\n",
    "              print_cm=True):\n",
    "    print('_' * 80)\n",
    "    print(\"Training: \")\n",
    "    print(clf)\n",
    "    t0 = time()\n",
    "    clf.fit(X_train, y_train)\n",
    "    train_time = time() - t0\n",
    "    print(\"train time: %0.3fs\" % train_time)\n",
    "\n",
    "    t0 = time()\n",
    "    pred = clf.predict(X_test)\n",
    "    test_time = time() - t0\n",
    "    print(\"test time:  %0.3fs\" % test_time)\n",
    "\n",
    "    score = metrics.accuracy_score(y_test, pred)\n",
    "    print(\"accuracy:   %0.3f\" % score)\n",
    "    #print(\"Accuracy: %0.3f (+/- %0.3f)\" % (score.mean(), score.std() * 2))\n",
    "\n",
    "    if hasattr(clf, 'coef_'):\n",
    "        print(\"dimensionality: %d\" % clf.coef_.shape[1])\n",
    "        print(\"density: %f\" % density(clf.coef_))\n",
    "\n",
    "        if print_top10 and feature_names is not None:\n",
    "            print(\"top 10 keywords per class:\")\n",
    "            for i, label in enumerate([\"Make Update\", \"Setup Printer\", \"Shutdown Computer\",\"Software Recommendation\", \"None\"]):\n",
    "                top10 = np.argsort(clf.coef_[i])[-10:]\n",
    "                print(trim(\"%s: %s\" % (label, \" \".join([feature_names[i] for i in top10]))))\n",
    "        print()\n",
    "\n",
    "    if print_report:\n",
    "        print(\"classification report:\")\n",
    "        print(metrics.classification_report(y_test, pred,\n",
    "                                            target_names=target_names))\n",
    "\n",
    "    if print_cm:\n",
    "        print(\"confusion matrix:\")\n",
    "        print(metrics.confusion_matrix(y_test, pred))\n",
    "\n",
    "    print()\n",
    "    clf_descr = str(clf).split('(')[0]\n",
    "    return clf_descr, score, train_time, test_time\n",
    "    #avg=cross_val_score(clf,corpus,y,cv=5)\n",
    "#print avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_results(results):\n",
    "    # make some plots\n",
    "    indices = np.arange(len(results))\n",
    "\n",
    "    results = [[x[i] for x in results] for i in range(4)]\n",
    "\n",
    "    clf_names, score, training_time, test_time = results\n",
    "    training_time = np.array(training_time) / np.max(training_time)\n",
    "    test_time = np.array(test_time) / np.max(test_time)\n",
    "\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.title(\"Score\")\n",
    "    plt.barh(indices, score, .2, label=\"score\", color='navy')\n",
    "    plt.barh(indices + .3, training_time, .2, label=\"training time\",\n",
    "             color='c')\n",
    "    plt.barh(indices + .6, test_time, .2, label=\"test time\", color='darkorange')\n",
    "    plt.yticks(())\n",
    "    plt.legend(loc='best')\n",
    "    plt.subplots_adjust(left=.25)\n",
    "    plt.subplots_adjust(top=.95)\n",
    "    plt.subplots_adjust(bottom=.05)\n",
    "\n",
    "    for i, c in zip(indices, clf_names):\n",
    "        plt.text(-.3, i, c)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_X_andy_from_split():\n",
    "    # train_corpus, y_train = split[\"train\"][\"X\"], split[\"train\"][\"y\"]\n",
    "    # test_corpus, y_test = split[\"test\"][\"X\"], split[\"test\"][\"y\"]\n",
    "    vectorizer, feature_names = get_vectorizer(X_train_raw, preprocessor=preprocess, tokenizer=tokenize)\n",
    "    \n",
    "    X_train = vectorizer.transform(X_train_raw).toarray()\n",
    "    X_test = vectorizer.transform(X_test_raw).toarray()\n",
    "    \n",
    "    #for i,label in enumerate(y_train):\n",
    "     #   if label == 4:\n",
    "      #      y_train = np.append(y_train,[4], axis=0)\n",
    "       #     X_train = np.append(X_train,X_train[i], axis=0)\n",
    "            \n",
    "    return X_train, y_train, X_test, y_test, feature_names\n",
    "#print feature_names\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN: [123 151  27 143  79 103  62 119  81 108   8  25  61 154 125  83 137 116\n",
      " 133  80  17  85   4  49  33 134  38 138 130  16 129 161   7  19 105  84\n",
      "  87  70 152  52 109  93 150   1 160 127  74 146  32 117   6  12  44  54\n",
      "  82] \n",
      "\n",
      " TEST: [149 114  43 111  46  57  26 148 113  28  13   0  21 145  90  76 115  22\n",
      "  40  39  58 112  94  42 124  65 110  23 147  89 156 107  88 142 141 118\n",
      "  11   9  60  24  37  14  63  66  64  51  41  36 140  78 101  98  72  47\n",
      "  35 153 102 128 104  75 131  59 106  20 132  10  97 144 159   3  91   5\n",
      " 126  18  55  96  15  95  50 136  56 157  48 158  73  34 120  77  53 100\n",
      " 155  69  68  86 122  99 139   2  45  30 121 135  67  29  92  71  31] \n",
      "\n",
      "\n",
      "TRAIN: [  6 102  51 151 123  56 104  77  70 100 149   2  73  94 113 147  60  97\n",
      "  26 111  11  95  37  58  12 161  92  86  18  14  52 121  89  34 144  41\n",
      " 127 134 132  96  83  47  88   8  63  55 131 159  38 118  40   5  31 135\n",
      "  66] \n",
      "\n",
      " TEST: [128 105  87  13 138  98  27  53 148 155   0 125  20  33 119   1  23 139\n",
      "  82  39   7  28  67  68 143 154  79  64 107  44 141 116 133  90  62   3\n",
      "  91 160  22  29 112 142  84  49 124  85 103 150  17  69 108 137  50 129\n",
      " 114   9 110  61   4  65 115  72  81 122 156 126  99  71 140 136 120  32\n",
      "  57  78  80 157  45  46 158 153  30  21  76  74  43  10  24 117 146 109\n",
      " 152 101  19  16  93  59  15 130  75  25 145  35  54 106  36  42  48] \n",
      "\n",
      "\n",
      "TRAIN: [ 62  17  67   7  71 117  95 128  13  21  44  14  43  52   4 159  89 145\n",
      " 121 124  86  22 106 104  72  29   1  50  41  15 123  74  47   9 109   5\n",
      "  36 135 116  12 154  48 120 158  97 129 147  73  82  91  64  16  60  66\n",
      " 144] \n",
      "\n",
      " TEST: [ 70  42  34 125 149 148 101  76 151 141  10   2  38   8  57  81 136 114\n",
      " 108 160  26  98  55 122  83 131  78  63  88  11  92  80  23 139 143 126\n",
      "   6 161  61  45 107  18 150 155 142 133  32  39 130 138 132 134  84 115\n",
      " 105 118  20  27  25 140  96  87 102  30  75  51 119  69  53 113 153  68\n",
      " 127  46  28 100 156 103  94  24  90  49  56   3 111  59  85  65  58 137\n",
      "  93 112  99   0  19 110  54 157  79  40  35 152  37  33 146  31  77] \n",
      "\n",
      "\n",
      "TRAIN: [ 91 119 113 139  23  65 100 109 146 157  37 128  19  45  83  67  31 133\n",
      "   5  32   6  25  63 158  96  20 125 138 115  22  78 118   4  36 161  73\n",
      "  48  34  77 130  13 101 160  57  81   2  50 152 106  84 103  89  93  82\n",
      "  30] \n",
      "\n",
      " TEST: [104 102 143 141 122   9 136 151 147   7  40  26  79  88  90  56 121 149\n",
      "  28   1 159  76  15  39 120   8 142  62  51 116  16 140 148  75  98 129\n",
      "  85  46  69 111  38  54 135 155  72 153  41  14 156  61  12  71  99  94\n",
      "  60 134  92 145 126  47  42 154  33  11  35  43  74  24  21 144  18  10\n",
      "  70 112  29  87   0 110  58  49  53 150  68 131  52  59  80  66 137  86\n",
      "  44  55 124  97 127 108   3 117  95 107 123 132  17 114  64  27 105] \n",
      "\n",
      "\n",
      "TRAIN: [ 80 128  60  72 106  45  86 129   8  37 134 144  36   9  14  11  34  84\n",
      "  10 158   3 109  58 116  21  12  43  55 142 117   4 124  91  62 149 159\n",
      "  90  75  77 154  99  56  22 150  96  59   0 126  82  66 140  28   6 104\n",
      "  29] \n",
      "\n",
      " TEST: [ 98 152 103  97  78  19 156  88  63  93  27 100 160  52 153  76  33 123\n",
      " 101  85  83  68   5 125 138  89  94  20  31 111  35  79  74  32  16 120\n",
      "  48 122 135 147  53 141 139 105 132  92 108 102 110  50  46  47   2 136\n",
      "  24  69  30 113  65  57  51  15 145 151  26 130 107  41 119 143  54 146\n",
      " 161  40 131  17  71   7 121  70 133  44  42  13 148 118  25 112  23 157\n",
      "  87 114  81  38   1  73 155  61 127  49 137  64  67  95  18 115  39] \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "splits = stratified_split(corpus, y, n_splits=5, test_size=0.66)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Split 0\n",
      "Train Size: 61\n",
      "Test Size: 109\n",
      "================================================================================\n",
      "Ridge Classifier\n",
      "________________________________________________________________________________\n",
      "Training: \n",
      "RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,\n",
      "        max_iter=None, normalize=False, random_state=None, solver='lsqr',\n",
      "        tol=0.01)\n",
      "train time: 0.059s\n",
      "test time:  0.022s\n",
      "accuracy:   0.890\n",
      "dimensionality: 4858\n",
      "density: 1.000000\n",
      "\n",
      "classification report:\n",
      "                         precision    recall  f1-score   support\n",
      "\n",
      "            Make Update       0.95      0.97      0.96        37\n",
      "          Setup Printer       0.93      1.00      0.96        13\n",
      "      Shutdown Computer       1.00      1.00      1.00        14\n",
      "Software Recommendation       0.94      0.75      0.83        40\n",
      "                   None       0.36      0.80      0.50         5\n",
      "\n",
      "            avg / total       0.92      0.89      0.90       109\n",
      "\n",
      "confusion matrix:\n",
      "[[36  0  0  1  0]\n",
      " [ 0 13  0  0  0]\n",
      " [ 0  0 14  0  0]\n",
      " [ 2  1  0 30  7]\n",
      " [ 0  0  0  1  4]]\n",
      "\n",
      "================================================================================\n",
      "gridsearchknn\n",
      "________________________________________________________________________________\n",
      "Training: \n",
      "GridSearchCV(cv=10, error_score='raise',\n",
      "       estimator=KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "           metric_params=None, n_jobs=1, n_neighbors=5, p=2,\n",
      "           weights='uniform'),\n",
      "       fit_params=None, iid=True, n_jobs=1,\n",
      "       param_grid={'n_neighbors': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]},\n",
      "       pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
      "       scoring=None, verbose=0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/insideout/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:73: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train time: 1.961s\n",
      "test time:  0.041s\n",
      "accuracy:   0.743\n",
      "classification report:\n",
      "                         precision    recall  f1-score   support\n",
      "\n",
      "            Make Update       0.91      0.86      0.89        37\n",
      "          Setup Printer       0.53      0.69      0.60        13\n",
      "      Shutdown Computer       0.67      0.86      0.75        14\n",
      "Software Recommendation       0.96      0.62      0.76        40\n",
      "                   None       0.23      0.60      0.33         5\n",
      "\n",
      "            avg / total       0.82      0.74      0.76       109\n",
      "\n",
      "confusion matrix:\n",
      "[[32  3  1  0  1]\n",
      " [ 0  9  1  0  3]\n",
      " [ 0  0 12  1  1]\n",
      " [ 3  3  4 25  5]\n",
      " [ 0  2  0  0  3]]\n",
      "\n",
      "================================================================================\n",
      "gridsearchmlp\n",
      "________________________________________________________________________________\n",
      "Training: \n",
      "GridSearchCV(cv=10, error_score='raise',\n",
      "       estimator=MLPClassifier(activation='tanh', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
      "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
      "       hidden_layer_sizes=(100,), learning_rate='constant',\n",
      "       learning_rate_init=0.001, max_iter=200, momentum=0.9,\n",
      "       nesterovs_momentum=True, power_t=0.5, random_state=None,\n",
      "       shuffle=True, solver='adam', tol=0.0001, validation_fraction=0.1,\n",
      "       verbose=False, warm_start=False),\n",
      "       fit_params=None, iid=True, n_jobs=1,\n",
      "       param_grid={'hidden_layer_sizes': [(100, 50), (300, 100, 50), (200, 100), (500, 300, 100, 50)]},\n",
      "       pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
      "       scoring=None, verbose=0)\n",
      "train time: 36.598s\n",
      "test time:  0.002s\n",
      "accuracy:   0.899\n",
      "classification report:\n",
      "                         precision    recall  f1-score   support\n",
      "\n",
      "            Make Update       0.92      0.95      0.93        37\n",
      "          Setup Printer       0.81      1.00      0.90        13\n",
      "      Shutdown Computer       0.93      1.00      0.97        14\n",
      "Software Recommendation       0.92      0.85      0.88        40\n",
      "                   None       0.67      0.40      0.50         5\n",
      "\n",
      "            avg / total       0.90      0.90      0.89       109\n",
      "\n",
      "confusion matrix:\n",
      "[[35  1  0  1  0]\n",
      " [ 0 13  0  0  0]\n",
      " [ 0  0 14  0  0]\n",
      " [ 3  2  0 34  1]\n",
      " [ 0  0  1  2  2]]\n",
      "\n",
      "================================================================================\n",
      "Passive-Aggressive\n",
      "________________________________________________________________________________\n",
      "Training: \n",
      "PassiveAggressiveClassifier(C=1.0, average=False, class_weight=None,\n",
      "              fit_intercept=True, loss='hinge', max_iter=50, n_iter=None,\n",
      "              n_jobs=1, random_state=None, shuffle=True, tol=None,\n",
      "              verbose=0, warm_start=False)\n",
      "train time: 0.154s\n",
      "test time:  0.001s\n",
      "accuracy:   0.917\n",
      "dimensionality: 4858\n",
      "density: 0.917497\n",
      "\n",
      "classification report:\n",
      "                         precision    recall  f1-score   support\n",
      "\n",
      "            Make Update       0.92      0.95      0.93        37\n",
      "          Setup Printer       1.00      1.00      1.00        13\n",
      "      Shutdown Computer       1.00      1.00      1.00        14\n",
      "Software Recommendation       0.88      0.90      0.89        40\n",
      "                   None       0.67      0.40      0.50         5\n",
      "\n",
      "            avg / total       0.91      0.92      0.91       109\n",
      "\n",
      "confusion matrix:\n",
      "[[35  0  0  2  0]\n",
      " [ 0 13  0  0  0]\n",
      " [ 0  0 14  0  0]\n",
      " [ 3  0  0 36  1]\n",
      " [ 0  0  0  3  2]]\n",
      "\n",
      "================================================================================\n",
      "gridsearchRF\n",
      "________________________________________________________________________________\n",
      "Training: \n",
      "GridSearchCV(cv=10, error_score='raise',\n",
      "       estimator=RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
      "            oob_score=False, random_state=None, verbose=0,\n",
      "            warm_start=False),\n",
      "       fit_params=None, iid=True, n_jobs=1,\n",
      "       param_grid={'n_estimators': [50, 60, 70], 'min_samples_leaf': [1, 2]},\n",
      "       pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
      "       scoring=None, verbose=0)\n",
      "train time: 2.739s\n",
      "test time:  0.003s\n",
      "accuracy:   0.917\n",
      "classification report:\n",
      "                         precision    recall  f1-score   support\n",
      "\n",
      "            Make Update       0.93      1.00      0.96        37\n",
      "          Setup Printer       0.92      0.92      0.92        13\n",
      "      Shutdown Computer       1.00      1.00      1.00        14\n",
      "Software Recommendation       0.92      0.88      0.90        40\n",
      "                   None       0.50      0.40      0.44         5\n",
      "\n",
      "            avg / total       0.91      0.92      0.91       109\n",
      "\n",
      "confusion matrix:\n",
      "[[37  0  0  0  0]\n",
      " [ 0 12  0  1  0]\n",
      " [ 0  0 14  0  0]\n",
      " [ 3  0  0 35  2]\n",
      " [ 0  1  0  2  2]]\n",
      "\n",
      "================================================================================\n",
      "L2 penalty\n",
      "________________________________________________________________________________\n",
      "Training: \n",
      "LinearSVC(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
      "     multi_class='ovr', penalty='l2', random_state=None, tol=0.001,\n",
      "     verbose=0)\n",
      "train time: 0.008s\n",
      "test time:  0.001s\n",
      "accuracy:   0.908\n",
      "dimensionality: 4858\n",
      "density: 1.000000\n",
      "\n",
      "classification report:\n",
      "                         precision    recall  f1-score   support\n",
      "\n",
      "            Make Update       0.95      0.95      0.95        37\n",
      "          Setup Printer       1.00      1.00      1.00        13\n",
      "      Shutdown Computer       1.00      1.00      1.00        14\n",
      "Software Recommendation       0.88      0.88      0.88        40\n",
      "                   None       0.40      0.40      0.40         5\n",
      "\n",
      "            avg / total       0.91      0.91      0.91       109\n",
      "\n",
      "confusion matrix:\n",
      "[[35  0  0  2  0]\n",
      " [ 0 13  0  0  0]\n",
      " [ 0  0 14  0  0]\n",
      " [ 2  0  0 35  3]\n",
      " [ 0  0  0  3  2]]\n",
      "\n",
      "________________________________________________________________________________\n",
      "Training: \n",
      "SGDClassifier(alpha=0.0001, average=False, class_weight=None, epsilon=0.1,\n",
      "       eta0=0.0, fit_intercept=True, l1_ratio=0.15,\n",
      "       learning_rate='optimal', loss='hinge', max_iter=50, n_iter=None,\n",
      "       n_jobs=1, penalty='l2', power_t=0.5, random_state=None,\n",
      "       shuffle=True, tol=None, verbose=0, warm_start=False)\n",
      "train time: 0.075s\n",
      "test time:  0.001s\n",
      "accuracy:   0.835\n",
      "dimensionality: 4858\n",
      "density: 0.608440\n",
      "\n",
      "classification report:\n",
      "                         precision    recall  f1-score   support\n",
      "\n",
      "            Make Update       0.83      0.92      0.87        37\n",
      "          Setup Printer       1.00      0.85      0.92        13\n",
      "      Shutdown Computer       0.68      0.93      0.79        14\n",
      "Software Recommendation       0.86      0.80      0.83        40\n",
      "                   None       1.00      0.20      0.33         5\n",
      "\n",
      "            avg / total       0.85      0.83      0.83       109\n",
      "\n",
      "confusion matrix:\n",
      "[[34  0  2  1  0]\n",
      " [ 1 11  0  1  0]\n",
      " [ 0  0 13  1  0]\n",
      " [ 5  0  3 32  0]\n",
      " [ 1  0  1  2  1]]\n",
      "\n",
      "================================================================================\n",
      "L1 penalty\n",
      "________________________________________________________________________________\n",
      "Training: \n",
      "LinearSVC(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
      "     multi_class='ovr', penalty='l1', random_state=None, tol=0.001,\n",
      "     verbose=0)\n",
      "train time: 0.027s\n",
      "test time:  0.001s\n",
      "accuracy:   0.899\n",
      "dimensionality: 4858\n",
      "density: 0.004446\n",
      "\n",
      "classification report:\n",
      "                         precision    recall  f1-score   support\n",
      "\n",
      "            Make Update       0.92      0.89      0.90        37\n",
      "          Setup Printer       0.93      1.00      0.96        13\n",
      "      Shutdown Computer       1.00      1.00      1.00        14\n",
      "Software Recommendation       0.86      0.90      0.88        40\n",
      "                   None       0.67      0.40      0.50         5\n",
      "\n",
      "            avg / total       0.90      0.90      0.90       109\n",
      "\n",
      "confusion matrix:\n",
      "[[33  1  0  3  0]\n",
      " [ 0 13  0  0  0]\n",
      " [ 0  0 14  0  0]\n",
      " [ 3  0  0 36  1]\n",
      " [ 0  0  0  3  2]]\n",
      "\n",
      "________________________________________________________________________________\n",
      "Training: \n",
      "SGDClassifier(alpha=0.0001, average=False, class_weight=None, epsilon=0.1,\n",
      "       eta0=0.0, fit_intercept=True, l1_ratio=0.15,\n",
      "       learning_rate='optimal', loss='hinge', max_iter=50, n_iter=None,\n",
      "       n_jobs=1, penalty='l1', power_t=0.5, random_state=None,\n",
      "       shuffle=True, tol=None, verbose=0, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/insideout/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:73: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n",
      "  DeprecationWarning)\n",
      "/home/insideout/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:73: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train time: 0.248s\n",
      "test time:  0.001s\n",
      "accuracy:   0.899\n",
      "dimensionality: 4858\n",
      "density: 0.665294\n",
      "\n",
      "classification report:\n",
      "                         precision    recall  f1-score   support\n",
      "\n",
      "            Make Update       0.93      1.00      0.96        37\n",
      "          Setup Printer       0.87      1.00      0.93        13\n",
      "      Shutdown Computer       0.78      1.00      0.88        14\n",
      "Software Recommendation       0.94      0.82      0.88        40\n",
      "                   None       1.00      0.20      0.33         5\n",
      "\n",
      "            avg / total       0.91      0.90      0.89       109\n",
      "\n",
      "confusion matrix:\n",
      "[[37  0  0  0  0]\n",
      " [ 0 13  0  0  0]\n",
      " [ 0  0 14  0  0]\n",
      " [ 3  1  3 33  0]\n",
      " [ 0  1  1  2  1]]\n",
      "\n",
      "================================================================================\n",
      "Elastic-Net penalty\n",
      "________________________________________________________________________________\n",
      "Training: \n",
      "SGDClassifier(alpha=0.0001, average=False, class_weight=None, epsilon=0.1,\n",
      "       eta0=0.0, fit_intercept=True, l1_ratio=0.15,\n",
      "       learning_rate='optimal', loss='hinge', max_iter=50, n_iter=None,\n",
      "       n_jobs=1, penalty='elasticnet', power_t=0.5, random_state=None,\n",
      "       shuffle=True, tol=None, verbose=0, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/insideout/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:73: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train time: 0.242s\n",
      "test time:  0.001s\n",
      "accuracy:   0.826\n",
      "dimensionality: 4858\n",
      "density: 0.600659\n",
      "\n",
      "classification report:\n",
      "                         precision    recall  f1-score   support\n",
      "\n",
      "            Make Update       0.94      0.84      0.89        37\n",
      "          Setup Printer       0.62      1.00      0.76        13\n",
      "      Shutdown Computer       0.74      1.00      0.85        14\n",
      "Software Recommendation       0.94      0.75      0.83        40\n",
      "                   None       0.50      0.40      0.44         5\n",
      "\n",
      "            avg / total       0.85      0.83      0.83       109\n",
      "\n",
      "confusion matrix:\n",
      "[[31  5  0  1  0]\n",
      " [ 0 13  0  0  0]\n",
      " [ 0  0 14  0  0]\n",
      " [ 2  2  4 30  2]\n",
      " [ 0  1  1  1  2]]\n",
      "\n",
      "================================================================================\n",
      "NearestCentroid (aka Rocchio classifier)\n",
      "________________________________________________________________________________\n",
      "Training: \n",
      "NearestCentroid(metric='euclidean', shrink_threshold=None)\n",
      "train time: 0.001s\n",
      "test time:  0.001s\n",
      "accuracy:   0.578\n",
      "classification report:\n",
      "                         precision    recall  f1-score   support\n",
      "\n",
      "            Make Update       0.87      0.73      0.79        37\n",
      "          Setup Printer       0.19      0.31      0.24        13\n",
      "      Shutdown Computer       0.77      0.71      0.74        14\n",
      "Software Recommendation       0.76      0.55      0.64        40\n",
      "                   None       0.00      0.00      0.00         5\n",
      "\n",
      "            avg / total       0.70      0.58      0.63       109\n",
      "\n",
      "confusion matrix:\n",
      "[[27  7  0  1  2]\n",
      " [ 2  4  2  4  1]\n",
      " [ 0  1 10  0  3]\n",
      " [ 2  7  0 22  9]\n",
      " [ 0  2  1  2  0]]\n",
      "\n",
      "================================================================================\n",
      "Naive Bayes\n",
      "________________________________________________________________________________\n",
      "Training: \n",
      "MultinomialNB(alpha=0.01, class_prior=None, fit_prior=True)\n",
      "train time: 0.002s\n",
      "test time:  0.001s\n",
      "accuracy:   0.908\n",
      "dimensionality: 4858\n",
      "density: 1.000000\n",
      "\n",
      "classification report:\n",
      "                         precision    recall  f1-score   support\n",
      "\n",
      "            Make Update       0.95      0.95      0.95        37\n",
      "          Setup Printer       0.81      1.00      0.90        13\n",
      "      Shutdown Computer       0.88      1.00      0.93        14\n",
      "Software Recommendation       0.92      0.90      0.91        40\n",
      "                   None       1.00      0.20      0.33         5\n",
      "\n",
      "            avg / total       0.92      0.91      0.90       109\n",
      "\n",
      "confusion matrix:\n",
      "[[35  1  1  0  0]\n",
      " [ 0 13  0  0  0]\n",
      " [ 0  0 14  0  0]\n",
      " [ 2  2  0 36  0]\n",
      " [ 0  0  1  3  1]]\n",
      "\n",
      "________________________________________________________________________________\n",
      "Training: \n",
      "BernoulliNB(alpha=0.01, binarize=0.0, class_prior=None, fit_prior=True)\n",
      "train time: 0.002s\n",
      "test time:  0.004s\n",
      "accuracy:   0.936\n",
      "dimensionality: 4858\n",
      "density: 1.000000\n",
      "\n",
      "classification report:\n",
      "                         precision    recall  f1-score   support\n",
      "\n",
      "            Make Update       0.95      0.97      0.96        37\n",
      "          Setup Printer       0.87      1.00      0.93        13\n",
      "      Shutdown Computer       1.00      1.00      1.00        14\n",
      "Software Recommendation       0.93      0.93      0.93        40\n",
      "                   None       1.00      0.40      0.57         5\n",
      "\n",
      "            avg / total       0.94      0.94      0.93       109\n",
      "\n",
      "confusion matrix:\n",
      "[[36  1  0  0  0]\n",
      " [ 0 13  0  0  0]\n",
      " [ 0  0 14  0  0]\n",
      " [ 2  1  0 37  0]\n",
      " [ 0  0  0  3  2]]\n",
      "\n",
      "================================================================================\n",
      "LinearSVC with L1-based feature selection\n",
      "________________________________________________________________________________\n",
      "Training: \n",
      "Pipeline(memory=None,\n",
      "     steps=[('feature_selection', SelectFromModel(estimator=LinearSVC(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
      "     multi_class='ovr', penalty='l1', random_state=None, tol=0.001,\n",
      "     verbose=0),\n",
      "        norm_order=1, prefit=...ax_iter=1000,\n",
      "     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
      "     verbose=0))])\n",
      "train time: 0.038s\n",
      "test time:  0.000s\n",
      "accuracy:   0.890\n",
      "classification report:\n",
      "                         precision    recall  f1-score   support\n",
      "\n",
      "            Make Update       0.94      0.89      0.92        37\n",
      "          Setup Printer       0.86      0.92      0.89        13\n",
      "      Shutdown Computer       1.00      1.00      1.00        14\n",
      "Software Recommendation       0.86      0.90      0.88        40\n",
      "                   None       0.50      0.40      0.44         5\n",
      "\n",
      "            avg / total       0.89      0.89      0.89       109\n",
      "\n",
      "confusion matrix:\n",
      "[[33  2  0  2  0]\n",
      " [ 0 12  0  1  0]\n",
      " [ 0  0 14  0  0]\n",
      " [ 2  0  0 36  2]\n",
      " [ 0  0  0  3  2]]\n",
      "\n",
      "================================================================================\n",
      "KMeans\n",
      "________________________________________________________________________________\n",
      "Training: \n",
      "KMeans(algorithm='auto', copy_x=True, init='k-means++', max_iter=300,\n",
      "    n_clusters=2, n_init=10, n_jobs=1, precompute_distances='auto',\n",
      "    random_state=0, tol=0.0001, verbose=0)\n",
      "train time: 0.043s\n",
      "test time:  0.002s\n",
      "accuracy:   0.275\n",
      "classification report:\n",
      "                         precision    recall  f1-score   support\n",
      "\n",
      "            Make Update       0.32      0.70      0.44        37\n",
      "          Setup Printer       0.14      0.31      0.20        13\n",
      "      Shutdown Computer       0.00      0.00      0.00        14\n",
      "Software Recommendation       0.00      0.00      0.00        40\n",
      "                   None       0.00      0.00      0.00         5\n",
      "\n",
      "            avg / total       0.13      0.28      0.17       109\n",
      "\n",
      "confusion matrix:\n",
      "[[26 11  0  0  0]\n",
      " [ 9  4  0  0  0]\n",
      " [13  1  0  0  0]\n",
      " [30 10  0  0  0]\n",
      " [ 3  2  0  0  0]]\n",
      "\n",
      "================================================================================\n",
      "LogisticRegression\n",
      "________________________________________________________________________________\n",
      "Training: \n",
      "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False)\n",
      "train time: 0.013s\n",
      "test time:  0.001s\n",
      "accuracy:   0.917\n",
      "dimensionality: 4858\n",
      "density: 1.000000\n",
      "\n",
      "classification report:\n",
      "                         precision    recall  f1-score   support\n",
      "\n",
      "            Make Update       0.94      0.92      0.93        37\n",
      "          Setup Printer       1.00      1.00      1.00        13\n",
      "      Shutdown Computer       1.00      1.00      1.00        14\n",
      "Software Recommendation       0.90      0.90      0.90        40\n",
      "                   None       0.50      0.60      0.55         5\n",
      "\n",
      "            avg / total       0.92      0.92      0.92       109\n",
      "\n",
      "confusion matrix:\n",
      "[[34  0  0  2  1]\n",
      " [ 0 13  0  0  0]\n",
      " [ 0  0 14  0  0]\n",
      " [ 2  0  0 36  2]\n",
      " [ 0  0  0  2  3]]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/insideout/anaconda3/lib/python3.6/site-packages/sklearn/utils/__init__.py:93: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
      "  if np.issubdtype(mask.dtype, np.int):\n",
      "/home/insideout/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAuMAAAI1CAYAAAB8GvSWAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ncUmSJKknJuOSJElST/4fP7XDnwTuYj4AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f689a834860>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "for i_s, split in enumerate(range(1)):\n",
    "    print(\"Evaluating Split {}\".format(i_s))\n",
    "    X_train, y_train, X_test, y_test, feature_names = get_X_andy_from_split()\n",
    "    target_names = [\"Make Update\", \"Setup Printer\", \"Shutdown Computer\",\"Software Recommendation\", \"None\"]\n",
    "    print(\"Train Size: {}\\nTest Size: {}\".format(X_train.shape[0], X_test.shape[0]))\n",
    "    results = []\n",
    "    #alphas = np.array([1,0.1,0.01,0.001,0.0001,0])\n",
    "    parameters_mlp={'hidden_layer_sizes':[(100,50),(300,100,50),(200,100),(500,300,100,50)]}\n",
    "    parameters_RF={ \"n_estimators\" : [50,60,70],\n",
    "           \"min_samples_leaf\" : [1, 2]}\n",
    "    k_range = list(range(1, 11))\n",
    "    parameters_knn = {'n_neighbors':k_range}\n",
    "    knn=KNeighborsClassifier(n_neighbors=5)\n",
    "    for clf, name in [  \n",
    "            (RidgeClassifier(tol=1e-2, solver=\"lsqr\"), \"Ridge Classifier\"),\n",
    "            (GridSearchCV(knn,parameters_knn, cv=10),\"gridsearchknn\"),\n",
    "            #(Perceptron(n_iter=50), \"Perceptron\"),\n",
    "            (GridSearchCV(MLPClassifier(activation='tanh'),parameters_mlp, cv=10),\"gridsearchmlp\"),\n",
    "           # (MLPClassifier(hidden_layer_sizes=(100, 50), activation=\"logistic\", max_iter=300), \"MLP\"),\n",
    "            #(MLPClassifier(hidden_layer_sizes=(300, 100, 50), activation=\"logistic\", max_iter=500), \"MLP\"),\n",
    "           # (MLPClassifier(hidden_layer_sizes=(300, 100, 50), activation=\"tanh\", max_iter=500), \"MLP\"),\n",
    "            (PassiveAggressiveClassifier(n_iter=50), \"Passive-Aggressive\"),\n",
    "           # (KNeighborsClassifier(n_neighbors=1), \"kNN\"),\n",
    "           # (KNeighborsClassifier(n_neighbors=3), \"kNN\"),\n",
    "           # (KNeighborsClassifier(n_neighbors=5), \"kNN\"),\n",
    "            #(KNeighborsClassifier(n_neighbors=10), \"kNN\"),\n",
    "            (GridSearchCV(RandomForestClassifier(n_estimators=10),parameters_RF, cv=10),\"gridsearchRF\")\n",
    "            #(RandomForestClassifier(n_estimators=10), \"Random forest\"),\n",
    "            #(RandomForestClassifier(n_estimators=50), \"Random forest\")\n",
    "    ]:\n",
    "           \n",
    "        print('=' * 80)\n",
    "        print(name)\n",
    "        results.append(benchmark(clf, X_train, y_train, X_test, y_test, target_names,\n",
    "                                 feature_names=feature_names))\n",
    "       # print('parameters')\n",
    "       # print(clf.grid_scores_[0])\n",
    "        #print('CV Validation Score')\n",
    "       # print(clf.grid_scores_[0].cv_validation_scores)\n",
    "       # print('Mean Validation Score')\n",
    "       # print(clf.grid_scores_[0].mean_validation_score)\n",
    "       # grid_mean_scores = [result.mean_validation_score for result in clf.grid_scores_]\n",
    "       # print(grid_mean_scores)\n",
    "       # plt.plot(k_range, grid_mean_scores)\n",
    "       # plt.xlabel('Value of K for KNN')\n",
    "       # plt.ylabel('Cross-Validated Accuracy')\n",
    "\n",
    "    #parameters_Linearsvc = [{'C': [1, 10], 'gamma': [0.1,1.0]}]\n",
    "    for penalty in [\"l2\", \"l1\"]:\n",
    "        print('=' * 80)\n",
    "        print(\"%s penalty\" % penalty.upper())\n",
    "        # Train Liblinear model\n",
    "        #grid=(GridSearchCV(LinearSVC,parameters_Linearsvc, cv=10),\"gridsearchSVC\")\n",
    "        #results.append(benchmark(LinearSVC(penalty=penalty), X_train, y_train, X_test, y_test, target_names,\n",
    "                                # feature_names=feature_names))\n",
    "        results.append(benchmark(LinearSVC(penalty=penalty, dual=False,tol=1e-3),\n",
    "                                 X_train, y_train, X_test, y_test, target_names,\n",
    "                                 feature_names=feature_names))\n",
    "\n",
    "        # Train SGD model\n",
    "        results.append(benchmark(SGDClassifier(alpha=.0001, n_iter=50,\n",
    "                                               penalty=penalty),\n",
    "                                 X_train, y_train, X_test, y_test, target_names,\n",
    "                                 feature_names=feature_names))\n",
    "\n",
    "    # Train SGD with Elastic Net penalty\n",
    "    print('=' * 80)\n",
    "    print(\"Elastic-Net penalty\")\n",
    "    results.append(benchmark(SGDClassifier(alpha=.0001, n_iter=50,\n",
    "                                           penalty=\"elasticnet\"),\n",
    "                             X_train, y_train, X_test, y_test, target_names,\n",
    "                             feature_names=feature_names))\n",
    "\n",
    "    # Train NearestCentroid without threshold\n",
    "    print('=' * 80)\n",
    "    print(\"NearestCentroid (aka Rocchio classifier)\")\n",
    "    results.append(benchmark(NearestCentroid(),\n",
    "                             X_train, y_train, X_test, y_test, target_names,\n",
    "                             feature_names=feature_names))\n",
    "\n",
    "    # Train sparse Naive Bayes classifiers\n",
    "    print('=' * 80)\n",
    "    print(\"Naive Bayes\")\n",
    "    results.append(benchmark(MultinomialNB(alpha=.01),\n",
    "                             X_train, y_train, X_test, y_test, target_names,\n",
    "                             feature_names=feature_names))\n",
    "    results.append(benchmark(BernoulliNB(alpha=.01),\n",
    "                             X_train, y_train, X_test, y_test, target_names,\n",
    "                             feature_names=feature_names))\n",
    "\n",
    "    print('=' * 80)\n",
    "    print(\"LinearSVC with L1-based feature selection\")\n",
    "    # The smaller C, the stronger the regularization.\n",
    "    # The more regularization, the more sparsity.\n",
    "    \n",
    "# uncommenting more parameters will give better exploring power but will\n",
    "# increase processing time in a combinatorial way\n",
    "    results.append(benchmark(Pipeline([\n",
    "                                  ('feature_selection', SelectFromModel(LinearSVC(penalty=\"l1\", dual=False,\n",
    "                                                                                  tol=1e-3))),\n",
    "                                  ('classification', LinearSVC(penalty=\"l2\"))]),\n",
    "                             X_train, y_train, X_test, y_test, target_names,\n",
    "                             feature_names=feature_names))\n",
    "   # print(grid.grid_scores_)\n",
    "   #KMeans clustering algorithm \n",
    "    print('=' * 80)\n",
    "    print(\"KMeans\")\n",
    "    results.append(benchmark(KMeans(n_clusters=2, init='k-means++', max_iter=300,\n",
    "                verbose=0, random_state=0, tol=1e-4),\n",
    "                             X_train, y_train, X_test, y_test, target_names,\n",
    "                             feature_names=feature_names))\n",
    "    \n",
    "   \n",
    "    \n",
    "    print('=' * 80)\n",
    "    print(\"LogisticRegression\")\n",
    "    #kfold = model_selection.KFold(n_splits=2, random_state=0)\n",
    "    #model = LinearDiscriminantAnalysis()\n",
    "    results.append(benchmark(LogisticRegression(C=1.0, class_weight=None, dual=False,\n",
    "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
    "          multi_class='ovr', n_jobs=1, penalty='l2', random_state=None,\n",
    "          solver='liblinear', tol=0.0001, verbose=0, warm_start=False),\n",
    "                             X_train, y_train, X_test, y_test, target_names,\n",
    "                             feature_names=feature_names))\n",
    "    \n",
    "    plot_results(results)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
